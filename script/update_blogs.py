import requests
import re
import feedparser
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ä»“åº“ README çš„ Raw åœ°å€
README_URL = "https://raw.githubusercontent.com/smallclover/HelloBlog/main/README.md"

# ä¼ªè£…æµè§ˆå™¨ Header
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

update_results = {}

def fetch_readme():
    try:
        resp = requests.get(README_URL, headers=HEADERS)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"è·å– README å¤±è´¥: {e}")
        return None

def parse_blog_list(content):
    """è§£æè¡¨æ ¼æå–åšå®¢ä¿¡æ¯"""
    print(f"DEBUG: è·å–åˆ°çš„ README é•¿åº¦ä¸º {len(content)} å­—ç¬¦")
    blogs = []
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        # åªè¦æ˜¯è¡¨æ ¼è¡Œä¸”ä¸æ˜¯åˆ†å‰²çº¿
        if line.startswith("|") and "---" not in line and "åšå®¢åç§°" not in line:
            # æå–æœ‰æ•ˆåˆ—ï¼ˆå»é™¤ç©ºå­—ç¬¦ä¸²ï¼‰
            cols = [c.strip() for c in line.split('|') if c.strip()]
            
            if len(cols) < 2: continue

            try:
                # ç¬¬1åˆ—æ˜¯åç§°ï¼Œç¬¬2åˆ—æ˜¯é“¾æ¥ï¼Œç¬¬9åˆ—æ˜¯RSS
                name_raw = cols[0]
                name = name_raw.replace('**', '').strip()
                
                link_raw = cols[1]
                link_match = re.search(r'\((http.*?)\)', link_raw)
                link = link_match.group(1) if link_match else cols[1] # ç®€å•å®¹é”™
                
                rss = None
                if len(cols) >= 9:
                    rss_raw = cols[8]
                    rss_match = re.search(r'\((http.*?)\)', rss_raw)
                    rss = rss_match.group(1) if rss_match else None
                
                if name and link:
                    blogs.append({"name": name, "link": link, "rss": rss})
            except Exception:
                continue
    return blogs

# --- æŠ“å–é€»è¾‘ (ä¿æŒä¸å˜) ---
def get_date_from_rss(rss_url):
    if not rss_url: return None
    try:
        feed = feedparser.parse(rss_url)
        if not feed.entries: return None
        dt = feed.entries[0].get('published_parsed') or feed.entries[0].get('updated_parsed')
        if dt: return time.strftime('%Y-%m-%d', dt)
    except: pass
    return None

def get_date_from_sitemap(site_url):
    sitemap_paths = ['/sitemap.xml', '/sitemap_index.xml', '/atom.xml']
    for path in sitemap_paths:
        target_url = urljoin(site_url, path)
        try:
            resp = requests.get(target_url, headers=HEADERS, timeout=5)
            if resp.status_code != 200: continue
            soup = BeautifulSoup(resp.content, 'xml')
            lastmods = soup.find_all('lastmod')
            dates = [lm.text[:10] for lm in lastmods]
            if dates:
                dates.sort(reverse=True)
                return dates[0]
        except: continue
    return None

def get_date_by_brute_force(site_url):
    # è¿™é‡Œç›´æ¥ä½¿ç”¨ requests å›é€€æ–¹æ¡ˆï¼Œçœç•¥ Playwright ä»¥ç®€åŒ–ä»£ç 
    try:
        resp = requests.get(site_url, headers=HEADERS, timeout=10)
        resp.encoding = resp.apparent_encoding
        html = resp.text
        
        pattern_common = r'(202[3-5])[-/.](0[1-9]|1[0-2])[-/.](0[1-9]|[12][0-9]|3[01])'
        pattern_cn = r'(202[3-5])å¹´(0?[1-9]|1[0-2])æœˆ(0?[1-9]|[12][0-9]|3[01])æ—¥'
        
        found_dates = set()
        for match in re.findall(pattern_common, html):
            found_dates.add(f"{match[0]}-{match[1]}-{match[2]}")
        for match in re.findall(pattern_cn, html):
            found_dates.add(f"{match[0]}-{match[1].zfill(2)}-{match[2].zfill(2)}")
            
        if not found_dates: return None
        sorted_dates = sorted(list(found_dates), reverse=True)
        return sorted_dates[0]
    except: return None

def check_update(blog):
    print(f"æ­£åœ¨æ£€æŸ¥: {blog['name']} ... ", end="", flush=True)
    
    date = get_date_from_rss(blog.get('rss'))
    if date: 
        print(f"[RSS] {date}")
        return date
        
    date = get_date_from_sitemap(blog['link'])
    if date:
        print(f"[Sitemap] {date}")
        return date
        
    date = get_date_by_brute_force(blog['link'])
    if date:
        print(f"[HTML] {date}")
        return date
        
    print("âŒ æ— æ³•è·å–")
    return "Unknown"

def calculate_status_string(date_str):
    if date_str == "Unknown": return 'âš« åœæ›´'
    try:
        last_update = datetime.strptime(date_str, '%Y-%m-%d').date()
        if last_update >= (datetime.now().date() - timedelta(days=90)):
            return 'ğŸ”¥ æ´»è·ƒ'
        else:
            return f"æœ€åæ›´æ–°{last_update.year}å¹´{last_update.month:02d}æœˆ"
    except: return 'âš« åœæ›´'

# --- å…³é”®ä¿®å¤ 1: è¡¨æ ¼é‡ç»„ ---
def update_readme_content(original_content, update_results):
    new_lines = []
    lines = original_content.split('\n')
    
    for line in lines:
        stripped = line.strip()
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ•°æ®è¡Œï¼šä»¥ | å¼€å¤´ï¼Œä¸”ä¸åŒ…å« --- åˆ†å‰²çº¿ï¼Œä¸”ä¸æ˜¯è¡¨å¤´
        if stripped.startswith('|') and '---' not in stripped and 'åšå®¢åç§°' not in stripped:
            
            # 1. æå–æ‰€æœ‰å•å…ƒæ ¼å†…å®¹ï¼ˆå»é™¤ç©ºå­—ç¬¦ä¸²ï¼Œé¿å… || é—®é¢˜ï¼‰
            cols = [c.strip() for c in stripped.split('|') if c.strip()]
            
            # ç¡®ä¿åˆ—æ•°è¶³å¤Ÿï¼ˆä½ çš„è¡¨æ ¼æœ‰9åˆ—ï¼‰
            if len(cols) >= 9:
                # ç¬¬1åˆ—æ˜¯åç§°
                name_raw = cols[0].replace('**', '').strip()
                
                # å¦‚æœè¯¥åšå®¢æœ‰æ›´æ–°ç»“æœ
                if name_raw in update_results:
                    new_status = calculate_status_string(update_results[name_raw])
                    # ç¬¬7åˆ—æ˜¯çŠ¶æ€ (ç´¢å¼•6)
                    cols[6] = new_status
                
                # 2. é‡æ–°æ‹¼æ¥è¡¨æ ¼è¡Œ
                # æ ¼å¼ï¼š| Col1 | Col2 | ... |
                # è¿™æ ·å¯ä»¥ä¿è¯å·¦å³ä¸¤è¾¹å„åªæœ‰ä¸€ä¸ª |ï¼Œä¸”å†…å®¹æœ‰ç©ºæ ¼ç¼“å†²
                new_line = "| " + " | ".join(cols) + " |"
                new_lines.append(new_line)
            else:
                # å¦‚æœåˆ—æ•°ä¸å¯¹ï¼ŒåŸæ ·æ”¾å›ï¼ˆå¯èƒ½æ˜¯ç ´æŸè¡Œï¼‰
                new_lines.append(line)
        else:
            # éè¡¨æ ¼è¡ŒåŸæ ·æ”¾å›
            new_lines.append(line)
            
    return '\n'.join(new_lines)

# --- å…³é”®ä¿®å¤ 2: æ—¶é—´æˆ³è¦†ç›– ---
def update_timestamp(content):
    now = datetime.now()
    # æ ¼å¼åŒ–æ—¶é—´
    current_time_str = now.strftime("%Y/%m/%d %H:%M")
    
    # æ„é€ æ ‡å‡†çš„ HTML æ ‡ç­¾
    new_html_line = f'<p align="center"><span>æ›´æ–°æ—¶é—´ï¼š{current_time_str}</span></p>'
    
    new_lines = []
    lines = content.split('\n')
    
    time_updated = False
    for line in lines:
        # åªè¦è¡Œé‡ŒåŒ…å« "æ›´æ–°æ—¶é—´"ï¼Œæˆ–è€…æ˜¯é‚£ä¸ªç ´æŸçš„ "P25/11/..."
        # æˆ‘ä»¬å°±ç›´æ¥æ•´è¡Œæ›¿æ¢æ‰ï¼Œç¡®ä¿ä¿®å¤æ ¼å¼
        if "æ›´æ–°æ—¶é—´" in line or (line.strip().startswith("P2") and "</span>" in line):
            new_lines.append(new_html_line)
            time_updated = True
        else:
            new_lines.append(line)
            
    return '\n'.join(new_lines)

def main():
    global update_results
    original_content = fetch_readme()
    if not original_content: return

    blogs = parse_blog_list(original_content)
    
    for blog in blogs: 
        last_update = check_update(blog)
        if last_update != "Unknown":
            update_results[blog['name']] = last_update

    print("\nå¼€å§‹æ›´æ–° README...")
    
    # 1. æ›´æ–°è¡¨æ ¼
    content_step1 = update_readme_content(original_content, update_results)
    
    # 2. æ›´æ–°æ—¶é—´æˆ³
    final_content = update_timestamp(content_step1)
    
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(final_content)
        
    print("âœ… README.md å·²ä¿®å¤å¹¶æ›´æ–°ã€‚")

if __name__ == "__main__":
    main()