import requests
import re
import feedparser
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ä»“åº“ README çš„ Raw åœ°å€
README_URL = "https://raw.githubusercontent.com/smallclover/HelloBlog/main/README.md"

# ä¼ªè£…æµè§ˆå™¨ Header
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨æŠ“å–ç»“æœ {åšå®¢åç§°: 'YYYY-MM-DD'}
update_results = {}

def fetch_readme():
    try:
        resp = requests.get(README_URL, headers=HEADERS)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"è·å– README å¤±è´¥: {e}")
        return None

def parse_blog_list(content):
    """è¡¨æ ¼è§£æå‡½æ•° (ä¿æŒä¸å˜ï¼Œå·²åœ¨ä½ é‚£è°ƒè¯•é€šè¿‡)"""
    print(f"DEBUG: è·å–åˆ°çš„ README é•¿åº¦ä¸º {len(content)} å­—ç¬¦")
    
    blogs = []
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        
        if not line.startswith("|") or "---" in line or "åšå®¢åç§°" in line and "é“¾æ¥" in line:
            continue
            
        # --- å¼€å§‹è§£ææ•°æ®è¡Œ ---
        cols = [c.strip() for c in line.split('|')]
        clean_cols = [c for c in cols if c]
        
        if len(clean_cols) < 2:
            continue

        try:
            name_raw = clean_cols[0]
            name = name_raw.replace('**', '').strip()
            
            link_raw = clean_cols[1]
            link_match = re.search(r'\((http.*?)\)', link_raw)
            if link_match:
                link = link_match.group(1)
            else:
                link_simple = re.search(r'(http[s]?://\S+)', link_raw)
                link = link_simple.group(1) if link_simple else None
            
            rss = None
            if len(clean_cols) >= 9:
                rss_raw = clean_cols[8]
                rss_match = re.search(r'\((http.*?)\)', rss_raw)
                if rss_match:
                    rss = rss_match.group(1)
            
            if name and link:
                blogs.append({
                    "name": name,
                    "link": link,
                    "rss": rss
                })
        except Exception as e:
            # print(f"DEBUG: è§£æè¡Œå‡ºé”™ '{line}': {e}") # é¿å…è¿‡å¤šæ‰“å°
            continue

    print(f"DEBUG: è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(blogs)} ä¸ªåšå®¢")
    return blogs

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ï¼ˆçœç•¥ï¼Œä¿æŒä¸å˜ï¼‰ ---
def get_date_from_rss(rss_url):
    # ... (ä¸ä½ æä¾›çš„ä»£ç ä¸€è‡´)
    if not rss_url: return None
    try:
        feed = feedparser.parse(rss_url)
        if not feed.entries: return None
        dt = feed.entries[0].get('published_parsed') or feed.entries[0].get('updated_parsed')
        if dt: return time.strftime('%Y-%m-%d', dt)
    except:
        pass
    return None

def get_date_from_sitemap(site_url):
    # ... (ä¸ä½ æä¾›çš„ä»£ç ä¸€è‡´)
    sitemap_paths = ['/sitemap.xml', '/sitemap_index.xml', '/atom.xml']
    for path in sitemap_paths:
        target_url = urljoin(site_url, path)
        try:
            resp = requests.get(target_url, headers=HEADERS, timeout=5)
            if resp.status_code != 200: continue
            soup = BeautifulSoup(resp.content, 'xml')
            lastmods = soup.find_all('lastmod')
            dates = []
            for lm in lastmods:
                text = lm.text[:10]
                dates.append(text)
            if dates:
                dates.sort(reverse=True)
                return dates[0]
        except:
            continue
    return None

def get_date_by_brute_force(site_url):
    # ... (ä¸ä½ æä¾›çš„ä»£ç ä¸€è‡´)
    html = None
    # å°è¯•ç”¨ requests è·å–åŸå§‹ HTML
    try:
        resp = requests.get(site_url, headers=HEADERS, timeout=10)
        resp.encoding = resp.apparent_encoding
        html = resp.text
    except Exception as e2:
        # print(f" Â  [Requests Error] {e2}") # é¿å…è¿‡å¤šæ‰“å°
        return None

    if not html:
        return None
    try:
        pattern_common = r'(202[3-5])[-/.](0[1-9]|1[0-2])[-/.](0[1-9]|[12][0-9]|3[01])'
        pattern_cn = r'(202[3-5])å¹´(0?[1-9]|1[0-2])æœˆ(0?[1-9]|[12][0-9]|3[01])æ—¥'
        pattern_cn_space = r'(202[3-5])\s*å¹´\s*(0?[1-9]|1[0-2])\s*æœˆ\s*(0?[1-9]|[12][0-9]|3[01])\s*æ—¥'

        found_dates = set()
        
        for match in re.findall(pattern_common, html):
            date_str = f"{match[0]}-{match[1]}-{match[2]}"
            found_dates.add(date_str)
        for match in re.findall(pattern_cn, html):
            year, month, day = match
            month = month.zfill(2)
            day = day.zfill(2)
            date_str = f"{year}-{month}-{day}"
            found_dates.add(date_str)
        for match in re.findall(pattern_cn_space, html):
            year, month, day = match
            month = month.zfill(2)
            day = day.zfill(2)
            date_str = f"{year}-{month}-{day}"
            found_dates.add(date_str)

        if not found_dates: return None

        sorted_dates = sorted(list(found_dates), reverse=True)
        latest_date = sorted_dates[0]

        current_year = datetime.now().year
        # ç®€å•è¿‡æ»¤æœªæ¥æ—¶é—´
        if int(latest_date.split('-')[0]) > current_year + 1:
            if len(sorted_dates) > 1: return sorted_dates[1]
            return None

        return latest_date

    except Exception:
        return None

def check_update(blog):
    print(f"æ­£åœ¨æ£€æŸ¥: {blog['name']} ... ", end="", flush=True)

    date = get_date_from_rss(blog.get('rss'))
    if date:
        print(f"[RSS] {date}")
        return date

    date = get_date_from_sitemap(blog['link'])
    if date:
        print(f"[Sitemap] {date}")
        return date
    
    date = get_date_by_brute_force(blog['link'])
    if date:
        print(f"[HTML] {date}")
        return date
        
    print("âŒ æ— æ³•è·å–")
    return "Unknown"
# ----------------------------------------------------------------

def calculate_status_string(date_str):
    # ... (ä¿æŒä¸å˜)
    if date_str == "Unknown":
        return 'âš« åœæ›´' 
        
    try:
        last_update_date = datetime.strptime(date_str, '%Y-%m-%d').date()
        today = datetime.now().date()
        three_months_ago = today - timedelta(days=90)

        if last_update_date >= three_months_ago:
            return 'ğŸ”¥ æ´»è·ƒ'
        else:
            return f"æœ€åæ›´æ–°{last_update_date.year}å¹´{last_update_date.month:02d}æœˆ"
            
    except ValueError:
        return 'âš« åœæ›´'

# --- é‡ç‚¹ä¿®æ­£å‡½æ•°ï¼šè§£å†³è¡¨æ ¼é”™ä½é—®é¢˜ ---
def update_readme_content(original_content, update_results):
    """
    éå†åŸå§‹ READMEï¼Œæ›¿æ¢è¡¨æ ¼ä¸­ç¬¬ 7 åˆ—ï¼ˆæ›´æ–°çŠ¶æ€ï¼‰çš„å†…å®¹ã€‚
    **ä½¿ç”¨æ­£åˆ™æ›¿æ¢æ¥ä¿æŒåŸå§‹è¡¨æ ¼çš„å¯¹é½å’Œç©ºæ ¼ã€‚**
    """
    new_lines = []
    
    # åŒ¹é… Name æ‰€åœ¨çš„åŠ ç²—æ ¼å¼ï¼Œç”¨äºè¯†åˆ«æ•°æ®è¡Œ
    name_pattern = r'\*\*([^\*]+)\*\*'
    
    for line in original_content.split('\n'):
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ•°æ®è¡Œ (ä¸åŒ…å« '---', ä¸åŒ…å« 'åšå®¢åç§°', ä»¥ | å¼€å¤´)
        if line.strip().startswith('|') and '---' not in line and 'åšå®¢åç§°' not in line:
            
            # 1. å°è¯•æå–åšå®¢åç§°ï¼Œç”¨äºæŸ¥æ‰¾æ›´æ–°ç»“æœ
            name_match = re.search(name_pattern, line)
            if not name_match:
                # å†æ¬¡å°è¯•ä¸åŠ ç²—çš„åç§°åŒ¹é…ï¼Œä»¥é˜²ä¸‡ä¸€
                cols_check = [c.strip() for c in line.split('|')]
                if len(cols_check) > 1:
                    name_raw = cols_check[1].strip()
                    name_raw = name_raw.replace('**', '').strip()
                else:
                    new_lines.append(line)
                    continue

            name_raw = name_match.group(1).strip() if name_match else name_raw

            if name_raw in update_results:
                
                # 2. è®¡ç®—æ–°çš„çŠ¶æ€å­—ç¬¦ä¸²
                date_str = update_results[name_raw]
                new_status = calculate_status_string(date_str)
                
                # 3. æ‰¾åˆ°å¹¶æ›¿æ¢çŠ¶æ€åˆ—
                # è¡¨æ ¼è¡Œç»“æ„ï¼š| Col 1 | Col 2 | Col 3 | Col 4 | Col 5 | Col 6 | Col 7 | Col 8 | Col 9 |
                # çŠ¶æ€åˆ—æ˜¯ç¬¬ 7 åˆ— (ç´¢å¼• 7)
                
                # ä½¿ç”¨éè´ªå©ªåŒ¹é…æ¥åˆ†å‰²è¡¨æ ¼å†…å®¹
                parts = line.split('|')
                
                # åŸå§‹è¡Œï¼š'' [0] | Col 1 [1] | Col 2 [2] | ... | Col 7 [7] | Col 8 [8] | Col 9 [9] | '' [10]
                # çŠ¶æ€åœ¨ç´¢å¼• 7
                if len(parts) > 7:
                    # è·å–ç¬¬ 7 åˆ—çš„åŸå§‹å†…å®¹ (åŒ…å«å¯¹é½ç©ºæ ¼)
                    old_status_raw = parts[7]
                    
                    # æ›¿æ¢å†…å®¹ï¼šç”¨æ–°çŠ¶æ€æ›¿æ¢åŸå§‹çŠ¶æ€ï¼ŒåŒæ—¶ä¿æŒä¸¤ä¾§çš„ç©ºæ ¼å’Œå¯¹é½
                    # ä¾‹å¦‚: '    ğŸŸ¡ å¶å°”æ›´æ–° ' -> '    ğŸ”¥ æ´»è·ƒ '
                    
                    # æ„é€ æ–°çš„ç¬¬ 7 åˆ—å†…å®¹ï¼š
                    # ç›®æ ‡ï¼šå°†æ–°çš„çŠ¶æ€å­—ç¬¦ä¸²å±…ä¸­æˆ–å·¦å¯¹é½å¡«å…¥åŸæ¥çš„é•¿åº¦ä¸­
                    
                    # ç®€å•ç²—æš´çš„æ–¹å¼ï¼šæ›¿æ¢æ‰ç¬¬ 7 åˆ—çš„å†…å®¹ï¼Œä¾èµ–æ¸²æŸ“å™¨å¯¹é½
                    # ä¸ºäº†å°½å¯èƒ½ä¿æŒåŸå§‹å¯¹é½ï¼Œæˆ‘ä»¬ç”¨åŸå†…å®¹çš„é•¿åº¦è¿›è¡Œå¡«å……ï¼ˆè¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼å€¼ï¼‰
                    new_cell_content = new_status
                    
                    # å°è¯•ä¿ç•™ä¸¤ä¾§ç©ºæ ¼ï¼ˆå¦‚æœåŸå§‹å†…å®¹æœ‰ï¼‰
                    left_padding = re.match(r'^\s*', old_status_raw).group(0)
                    right_padding = re.search(r'\s*$', old_status_raw).group(0)
                    
                    parts[7] = f"{left_padding}{new_status}{right_padding}"
                    
                    # é‡æ–°æ‹¼æ¥è¡Œï¼Œæ³¨æ„ï¼šjoin ä» parts[1] åˆ° parts[-2]
                    # å¹¶åœ¨é¦–å°¾åŠ ä¸Š |
                    new_line = '|' + '|'.join(parts[1:-1]) + '|'
                    new_lines.append(new_line)
                    continue # å·²å¤„ç†ï¼Œè·³è¿‡åç»­
        
        # éæ•°æ®è¡Œï¼ˆæ ‡é¢˜ã€åˆ†éš”çº¿ã€éè¡¨æ ¼å†…å®¹ç­‰ï¼‰ä¿æŒä¸å˜
        new_lines.append(line)

    return '\n'.join(new_lines)

# --- é‡ç‚¹ä¿®æ­£å‡½æ•°ï¼šè§£å†³æ—¶é—´æˆ³æ ¼å¼é—®é¢˜ ---
def update_timestamp(content):
    """
    æ›´æ–° README é¡¶éƒ¨çš„â€œæ›´æ–°æ—¶é—´â€ã€‚
    ä¿®æ­£ï¼šæ—¶é—´æ ¼å¼ä¸º YYYY/MM/DD HH:MMã€‚
    """
    
    # ä¿®æ­£æ—¶é—´æ ¼å¼ï¼šåŒ¹é… README ä¸­çš„æ ¼å¼ YYYY/MM/DD HH:MM
    now = datetime.now()
    # å¦‚æœåœ¨ GitHub Actions ä¸­è¿è¡Œï¼Œé€šå¸¸éœ€è¦åŠ ä¸Šæ—¶åŒºè°ƒæ•´ï¼Œä½†è¿™é‡Œå…ˆä¿æŒç®€å•çš„æœ¬åœ°æ—¶é—´æ ¼å¼
    current_time_str = now.strftime("%Y/%m/%d %H:%M") 
    
    # æ­£åˆ™è¡¨è¾¾å¼ï¼šæŸ¥æ‰¾ "æ›´æ–°æ—¶é—´ï¼š" åé¢çš„æ—¥æœŸå’Œæ—¶é—´
    # ç›®æ ‡ï¼šåŒ¹é…å¹¶æ›¿æ¢ 'æ›´æ–°æ—¶é—´ï¼š' åé¢çš„æ‰€æœ‰å†…å®¹ç›´åˆ°è¡Œå°¾
    
    # ä¿®æ­£ä½ çš„ README ç»“æ„ï¼šå®ƒå¯èƒ½ä¸åœ¨ <p> æˆ– <span> é‡Œï¼Œè€Œæ˜¯åœ¨ Markdown æ–‡æœ¬ä¸­
    # å‡è®¾å®ƒæ˜¯è¿™æ ·çš„ä¸€è¡Œï¼šæ›´æ–°æ—¶é—´ï¼š2025/11/25 18:00
    
    # å°è¯•åŒ¹é… "æ›´æ–°æ—¶é—´ï¼š" è¿™ä¸€å¥
    pattern = r'(æ›´æ–°æ—¶é—´ï¼š).*?$'
    replacement = r'\1' + current_time_str
    
    print(f"DEBUG: æ­£åœ¨å°è¯•æ›´æ–°æ—¶é—´æˆ³ä¸º: {current_time_str}")
    
    # é€è¡Œæ£€æŸ¥å¹¶æ›¿æ¢ (ä¸ä½¿ç”¨ re.DOTALL)
    updated_lines = []
    replaced = False
    
    for line in content.split('\n'):
        if 'æ›´æ–°æ—¶é—´ï¼š' in line:
            # æ‰¾åˆ°ç›®æ ‡è¡Œï¼Œè¿›è¡Œæ›¿æ¢
            new_line = re.sub(pattern, replacement, line, flags=re.MULTILINE)
            updated_lines.append(new_line)
            replaced = True
        else:
            updated_lines.append(line)
            
    # å¦‚æœä½ çš„æ—¶é—´æˆ³æ˜¯ HTML æ ¼å¼ (å¦‚ä½ ä»£ç ä¸­çš„åŸå§‹æ­£åˆ™æ‰€ç¤º)ï¼Œè¯·æ”¹å›ï¼š
    # pattern_html = r'(<p\s+align="center">\s*<span>æ›´æ–°æ—¶é—´ï¼š).*?(</span>\s*</p>)'
    # updated_content = re.sub(pattern_html, replacement_html, content, flags=re.DOTALL)
    
    if not replaced:
        print("DEBUG: æœªæ‰¾åˆ° 'æ›´æ–°æ—¶é—´ï¼š' æ ‡ç­¾ï¼Œæ—¶é—´æˆ³æœªæ›´æ–°ã€‚")

    return '\n'.join(updated_lines)

# --- ä¸»å‡½æ•°ä¿æŒä¸å˜ ---
def main():
    global update_results
    
    # 1. è·å–åŸå§‹ README å†…å®¹
    original_content = fetch_readme()
    if not original_content: return

    # 2. è§£æåšå®¢åˆ—è¡¨
    blogs = parse_blog_list(original_content)
    # print(f"æ‰¾åˆ° {len(blogs)} ä¸ªåšå®¢ï¼Œå¼€å§‹æ£€æŸ¥æ›´æ–°...\n")
    
    # 3. æŠ“å–å¹¶å­˜å‚¨æ›´æ–°æ—¥æœŸ
    for blog in blogs:
        last_update = check_update(blog)
        if last_update != "Unknown":
            update_results[blog['name']] = last_update
        
    print("\n" + "="*50)
    print(f"æ£€æŸ¥å®Œæˆï¼Œå…±æˆåŠŸè·å– {len(update_results)} ä¸ªåšå®¢çš„æ›´æ–°æ—¶é—´ã€‚")
    print("="*50)

    # 4. æ›´æ–° README å†…å®¹ (è¡¨æ ¼çŠ¶æ€)
    updated_content_table = original_content
    if update_results:
        print("å¼€å§‹æ›´æ–°è¡¨æ ¼çŠ¶æ€...")
        updated_content_table = update_readme_content(original_content, update_results)
    
    # 5. æ›´æ–°æ—¶é—´æˆ³
    print("å¼€å§‹æ›´æ–°é¡¶éƒ¨çš„è¿è¡Œæ—¶é—´æˆ³...")
    final_content = update_timestamp(updated_content_table)
    
    # 6. è¦†ç›–å†™å…¥åŸå§‹æ–‡ä»¶
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(final_content)
        
    print("\nâœ… README.md å·²æ›´æ–°ã€‚")

if __name__ == "__main__":
    # æ³¨æ„ï¼šè¯·ç¡®ä¿å·²ç»å®‰è£…äº† requests, feedparser, beautifulsoup4
    # å¦‚æœéœ€è¦ Playwright çš„æ€§èƒ½ï¼Œè¯·ç¡®ä¿å®‰è£…å¹¶åˆå§‹åŒ– Playwrightï¼š
    # pip install playwright && playwright install
    main()