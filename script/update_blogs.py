import requests
import re
import feedparser
import time
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import timedelta

# ä»“åº“ README çš„ Raw åœ°å€
README_URL = "https://raw.githubusercontent.com/smallclover/HelloBlog/main/README.md"

# ä¼ªè£…æµè§ˆå™¨ Headerï¼Œé˜²æ­¢è¢«æ‹¦æˆª
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

update_results = {}

def fetch_readme():
    try:
        resp = requests.get(README_URL, headers=HEADERS)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"è·å– README å¤±è´¥: {e}")
        return None
def parse_blog_list(content):
    """è¡¨æ ¼è§£æå‡½æ•°ï¼Œå¸¦è°ƒè¯•ä¿¡æ¯"""
    print(f"DEBUG: è·å–åˆ°çš„ README é•¿åº¦ä¸º {len(content)} å­—ç¬¦")
    
    blogs = []
    lines = content.split('\n')
    
    # æ‰¾åˆ°è¡¨æ ¼çš„åˆ—ç´¢å¼•æ˜ å°„
    # å‡è®¾æ ‡å‡†ç»“æ„: | åšå®¢åç§° | é“¾æ¥ | ... | RSS |
    # å®é™…ä¸Šæˆ‘ä»¬åªéœ€è¦ç¡®å®š Name(ç¬¬1åˆ—), Link(ç¬¬2åˆ—), RSS(ç¬¬9åˆ—)
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        # 1. è·³è¿‡éè¡¨æ ¼è¡Œ
        if not line.startswith("|"):
            continue
            
        # 2. è·³è¿‡åˆ†å‰²çº¿ (---|---)
        if "---" in line:
            continue
            
        # 3. è·³è¿‡è¡¨å¤´ (åŒ…å« "åšå®¢åç§°" å­—æ ·)
        if "åšå®¢åç§°" in line and "é“¾æ¥" in line:
            print(f"DEBUG: è·³è¿‡è¡¨å¤´è¡Œ: {line[:30]}...")
            continue
            
        # --- å¼€å§‹è§£ææ•°æ®è¡Œ ---
        
        # æŒ‰ | åˆ†å‰²ï¼Œå¹¶å»é™¤æ¯ä¸€é¡¹çš„é¦–å°¾ç©ºæ ¼
        cols = [c.strip() for c in line.split('|')]
        
        # split('|') åï¼Œå¦‚æœè¡Œé¦–å°¾éƒ½æœ‰ |ï¼Œåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªå…ƒç´ é€šå¸¸æ˜¯ç©ºå­—ç¬¦ä¸²
        # ä¾‹å¦‚: "| A | B |" -> ['', 'A', 'B', '']
        # å»é™¤ç©ºå­—ç¬¦ä¸²ï¼Œä¿ç•™æœ‰æ•ˆå†…å®¹
        clean_cols = [c for c in cols if c]
        
        # ç°åœ¨çš„ clean_colsç´¢å¼•: 0=åç§°, 1=é“¾æ¥, ..., 8=RSS (å¦‚æœæ²¡ç¼ºåˆ—)
        if len(clean_cols) < 2:
            # åˆ—å¤ªå°‘ï¼Œè‚¯å®šä¸æ˜¯æœ‰æ•ˆæ•°æ®
            continue

        try:
            # --- æå–åç§° ---
            # æ ¼å¼å¯èƒ½æ˜¯ "**Name**" æˆ– "Name"
            name_raw = clean_cols[0]
            # å»é™¤ Markdown åŠ ç²—ç¬¦å·
            name = name_raw.replace('**', '').strip()
            
            # --- æå–é“¾æ¥ ---
            # æ ¼å¼é€šå¸¸æ˜¯ "[url](url)"
            link_raw = clean_cols[1]
            link_match = re.search(r'\((http.*?)\)', link_raw)
            if link_match:
                link = link_match.group(1)
            else:
                # å°è¯•ç›´æ¥åŒ¹é… httpï¼Œé˜²æ­¢æœ‰äº›äººç›´æ¥å†™é“¾æ¥æ²¡åŠ  []()
                link_simple = re.search(r'(http[s]?://\S+)', link_raw)
                link = link_simple.group(1) if link_simple else None
            
            # --- æå– RSS (å‡è®¾åœ¨æœ€åä¸€åˆ— æˆ–è€… ç¬¬9åˆ—) ---
            rss = None
            # ä½ çš„è¡¨æ ¼å¤§æ¦‚æœ‰9åˆ—æ•°æ®ã€‚RSSåœ¨æœ€åä¸€åˆ—ã€‚
            # æ£€æŸ¥æ˜¯å¦æœ‰ RSS åˆ— (é€šå¸¸æ˜¯æœ€åä¸€åˆ—ï¼Œæˆ–è€…åŒ…å« 'feed'/'rss'/'xml' çš„é“¾æ¥)
            if len(clean_cols) >= 9:
                rss_raw = clean_cols[8] # ç¬¬9åˆ—
                rss_match = re.search(r'\((http.*?)\)', rss_raw)
                if rss_match:
                    rss = rss_match.group(1)
            
            # å¦‚æœè§£æåˆ°äº†åå­—å’Œé“¾æ¥ï¼Œå°±å­˜å…¥ç»“æœ
            if name and link:
                # print(f"DEBUG: æˆåŠŸè§£æ - {name}") # å¦‚æœå¤ªå¤šå¯ä»¥æ³¨é‡Šæ‰
                blogs.append({
                    "name": name,
                    "link": link,
                    "rss": rss
                })
                
        except Exception as e:
            print(f"DEBUG: è§£æè¡Œå‡ºé”™ '{line}': {e}")
            continue

    print(f"DEBUG: è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(blogs)} ä¸ªåšå®¢")
    return blogs

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ ---

def get_date_from_rss(rss_url):
    """ç­–ç•¥1: é€šè¿‡ RSS è·å–"""

    print(f"DEBUG: å°è¯•ä» rss è·å–æ—¥æœŸ: {rss_url}")

    if not rss_url: return None
    try:
        feed = feedparser.parse(rss_url)
        if not feed.entries: return None
        dt = feed.entries[0].get('published_parsed') or feed.entries[0].get('updated_parsed')
        if dt: return time.strftime('%Y-%m-%d', dt)
    except:
        pass
    return None

def get_date_from_sitemap(site_url):
    """ç­–ç•¥2: çŒœæµ‹å¹¶è¯»å– sitemap.xml"""
    print(f"DEBUG: å°è¯•ä» sitemap.xml è·å–æ—¥æœŸ: {site_url}")
    # å¸¸è§çš„ sitemap åœ°å€
    sitemap_paths = ['/sitemap.xml', '/sitemap_index.xml', '/atom.xml']
    
    for path in sitemap_paths:
        target_url = urljoin(site_url, path)
        try:
            resp = requests.get(target_url, headers=HEADERS, timeout=5)
            if resp.status_code != 200: continue
            
            # ç®€å•è§£æ XML å¯»æ‰¾ <lastmod>
            soup = BeautifulSoup(resp.content, 'xml')
            lastmods = soup.find_all('lastmod')
            dates = []
            for lm in lastmods:
                text = lm.text[:10] # æˆªå– YYYY-MM-DD
                dates.append(text)
            
            if dates:
                dates.sort(reverse=True) # æ’åºå–æœ€æ–°çš„
                return dates[0]
        except:
            continue
    return None
def get_date_by_brute_force(site_url):
    """
    ä½¿ç”¨ headless æµè§ˆå™¨ï¼ˆPlaywrightï¼‰æ¸²æŸ“é¡µé¢ï¼Œç„¶ååœ¨æ¸²æŸ“åçš„ HTML ä¸­æœç´¢æ—¥æœŸå­—ç¬¦ä¸²ã€‚
    å¦‚æœ Playwright ä¸å¯ç”¨æˆ–æ¸²æŸ“å¤±è´¥ï¼Œåˆ™å›é€€åˆ° requests è·å– HTML çš„æ–¹å¼ã€‚
    æ”¯æŒæ ¼å¼ï¼š2024-11-25, 2024/11/25, 2024.11.25, 2024å¹´11æœˆ25æ—¥
    """
    print(f"   [æš´åŠ›æœç´¢] æ­£åœ¨ä½¿ç”¨ headless æ¸²æŸ“: {site_url} ...")
    html = None

    # å…ˆå°è¯•ç”¨ Playwright æ¸²æŸ“ï¼ˆæ›´å¯é ï¼šèƒ½æŠ“åˆ° JS æ¸²æŸ“çš„å†…å®¹ï¼‰
    try:
        from playwright.sync_api import sync_playwright
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page(extra_http_headers={"User-Agent": HEADERS["User-Agent"]})
            # å°è¯•è·³è½¬å¹¶ç­‰å¾…ç½‘ç»œç©ºé—²ï¼ˆæœ€å¤š 15sï¼‰
            try:
                page.goto(site_url, timeout=15000, wait_until='networkidle')
            except Exception:
                # å¦‚æœ networkidle è¶…æ—¶ï¼Œåˆ™å°è¯• load
                try:
                    page.goto(site_url, timeout=15000, wait_until='load')
                except Exception as e:
                    print(f"   [Playwright goto Error] {e}")
            # å¯é€‰åœ°ç­‰å¾…ä¸€äº›å¸¸è§å…ƒç´ åŠ è½½ï¼ˆè¿™é‡Œä¸å¼ºåˆ¶ï¼‰
            html = page.content()
            browser.close()
            print("   [Info] Playwright æ¸²æŸ“æˆåŠŸï¼Œå–å¾—é¡µé¢å†…å®¹é•¿åº¦:", len(html) if html else 0)
    except Exception as e:
        print(f"   [Playwright Unavailable or Error] {e}")
        # å›é€€ï¼šç”¨ requests è·å–åŸå§‹ HTMLï¼ˆå¯èƒ½æŠ“ä¸åˆ° JS æ¸²æŸ“å†…å®¹ï¼‰
        try:
            resp = requests.get(site_url, headers=HEADERS, timeout=10)
            resp.encoding = resp.apparent_encoding
            html = resp.text
            print("   [Info] å›é€€åˆ° requests è·å– HTMLï¼Œé•¿åº¦:", len(html) if html else 0)
        except Exception as e2:
            print(f"   [Requests Error] {e2}")
            return None

    if not html:
        return None

    # ä¸‹é¢æ˜¯æ­£åˆ™åŒ¹é…é€»è¾‘ï¼ˆå’Œä½ åŸæ¥çš„å®ç°ç±»ä¼¼ï¼Œä½†æ›´ç¨³å¥ï¼‰
    try:
        # æ¨¡å¼ A: çº¯æ•°å­—åˆ†éš” (YYYY-MM-DD / YYYY/MM/DD / YYYY.MM.DD)
        pattern_common = r'(202[3-5])[-/.](0[1-9]|1[0-2])[-/.](0[1-9]|[12][0-9]|3[01])'
        # æ¨¡å¼ B: ä¸­æ–‡æ ¼å¼ (2024å¹´5æœˆ20æ—¥ / 2024å¹´05æœˆ20æ—¥)
        pattern_cn = r'(202[3-5])å¹´(0?[1-9]|1[0-2])æœˆ(0?[1-9]|[12][0-9]|3[01])æ—¥'

        found_dates = set()

        for match in re.findall(pattern_common, html):
            date_str = f"{match[0]}-{match[1]}-{match[2]}"
            found_dates.add(date_str)

        for match in re.findall(pattern_cn, html):
            year, month, day = match
            month = month.zfill(2)
            day = day.zfill(2)
            date_str = f"{year}-{month}-{day}"
            found_dates.add(date_str)

        if not found_dates:
            # è¿›ä¸€æ­¥å°è¯•åŒ¹é…ç±»ä¼¼ "2024å¹´ 05 æœˆ 20 æ—¥" å¸¦ç©ºæ ¼çš„ä¸­æ–‡æ ¼å¼
            pattern_cn_space = r'(202[3-5])\s*å¹´\s*(0?[1-9]|1[0-2])\s*æœˆ\s*(0?[1-9]|[12][0-9]|3[01])\s*æ—¥'
            for match in re.findall(pattern_cn_space, html):
                year, month, day = match
                month = month.zfill(2)
                day = day.zfill(2)
                date_str = f"{year}-{month}-{day}"
                found_dates.add(date_str)

        if not found_dates:
            return None

        sorted_dates = sorted(list(found_dates), reverse=True)
        latest_date = sorted_dates[0]

        current_year = datetime.now().year
        if int(latest_date.split('-')[0]) > current_year + 1:
            if len(sorted_dates) > 1:
                return sorted_dates[1]
            return None

        return latest_date

    except Exception as e:
        print(f"   [Regex Error] {e}")
        return None

def check_update(blog):
    # 1. ä¼˜å…ˆ RSS (æœ€å‡†)
    if blog.get('rss'):
        date = get_date_from_rss(blog['rss'])
        if date: return date

    # 2. å…¶æ¬¡ Sitemap (é€šå¸¸å¾ˆå‡†)
    date = get_date_from_sitemap(blog['link'])
    if date: return date
    
    # 3. æœ€åï¼šæš´åŠ›æœ HTML (ç¨å¾®æ…¢ç‚¹ï¼Œä½†èƒ½å…œåº•)
    # ç›´æ¥è°ƒç”¨ä¸Šé¢å†™çš„æ–°å‡½æ•°
    date = get_date_by_brute_force(blog['link'])
    if date: 
        return date # ä¸éœ€è¦æ‰“å° [HTML]ï¼Œå‡½æ•°é‡Œå·²ç»æ‰“å°äº†
        
    return "Unknown"

def calculate_status_string(date_str):
    """
    æ ¹æ®æ—¥æœŸè®¡ç®—æ–°çš„â€œæ›´æ–°çŠ¶æ€â€å­—ç¬¦ä¸²ã€‚
    å¦‚æœä¸‰ä¸ªæœˆå†…æœ‰æ›´æ–° -> 'ğŸ”¥ æ´»è·ƒ'
    å¦åˆ™ -> 'æœ€åæ›´æ–°YYYYå¹´MMæœˆ'
    """
    if date_str == "Unknown":
        return 'âš« åœæ›´' # å¦‚æœæŠ“å–å¤±è´¥ï¼Œæ˜¾ç¤ºåœæ›´æˆ–ä¿æŒåŸçŠ¶ï¼ˆè¿™é‡Œé€‰æ‹©æ˜¾ç¤ºåœæ›´ï¼‰
        
    try:
        # å°†æŠ“å–çš„æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸º datetime å¯¹è±¡
        last_update_date = datetime.strptime(date_str, '%Y-%m-%d').date()
        today = datetime.now().date()
        
        # å®šä¹‰ä¸‰ä¸ªæœˆå‰çš„æ—¥æœŸï¼ˆçº¦90å¤©ï¼‰
        three_months_ago = today - timedelta(days=90)

        if last_update_date >= three_months_ago:
            return 'ğŸ”¥ æ´»è·ƒ'
        else:
            # æ ¼å¼åŒ–ä¸º XXXXå¹´XXæœˆ
            return f"æœ€åæ›´æ–°{last_update_date.year}å¹´{last_update_date.month:02d}æœˆ"
            
    except ValueError:
        return 'âš« åœæ›´'

def update_readme_content(original_content, update_results):
    """
    éå†åŸå§‹ READMEï¼Œæ›¿æ¢è¡¨æ ¼ä¸­ç¬¬ 7 åˆ—ï¼ˆæ›´æ–°çŠ¶æ€ï¼‰çš„å†…å®¹ã€‚
    """
    new_lines = []
    
    # åŒ¹é… Markdown è¡¨æ ¼è¡Œçš„é€šç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºè¯†åˆ«æ•°æ®è¡Œ
    # æ³¨æ„ï¼šMarkdown è¡¨æ ¼è¡Œé€šå¸¸ä»¥ | å¼€å¤´
    # æˆ‘ä»¬è¦åŒ¹é…å¹¶ä¿ç•™ | Name | Link | Content | Author | Tags | Access | [Status] | Rec | RSS |
    # æ›¿æ¢ç›®æ ‡æ˜¯ Status æ‰€åœ¨çš„å†…å®¹
    
    for line in original_content.split('\n'):
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ•°æ®è¡Œ (ä¸åŒ…å« '---', ä¸åŒ…å« 'åšå®¢åç§°', ä»¥ | å¼€å¤´)
        if line.strip().startswith('|') and '---' not in line and 'åšå®¢åç§°' not in line:
            
            cols = [c.strip() for c in line.split('|')]
            # ç¡®ä¿åˆ—æ•°è¶³å¤Ÿ
            if len(cols) < 10: 
                new_lines.append(line)
                continue
            
            # æå–åç§° (ä»ç¬¬ 1 åˆ—)
            # åå­—åœ¨ç¬¬ 1 åˆ—ï¼Œå¯èƒ½åŒ…å« **åŠ ç²—**
            name_raw = cols[1].replace('**', '').strip()
            
            if name_raw in update_results:
                
                # 1. è®¡ç®—æ–°çš„çŠ¶æ€å­—ç¬¦ä¸²
                date_str = update_results[name_raw]
                new_status = calculate_status_string(date_str)
                
                # 2. æ„é€ æ–°çš„è¡Œ
                # æ›´æ–°çŠ¶æ€åœ¨ç¬¬ 7 åˆ— (cols åˆ—è¡¨ç´¢å¼• 7)
                
                # æ›¿æ¢å‰éœ€è¦å¤„ç† cols[7] çš„å†…å®¹ï¼Œé¿å…å½±å“å…¶ä»–åˆ—çš„å¯¹é½
                old_status_raw = cols[7]
                
                # ç¡®ä¿æ›¿æ¢åçš„å†…å®¹ä¸ä¼šå¤ªé•¿ï¼Œå¯¼è‡´åˆ—å¯¹é½å‡ºé—®é¢˜ï¼Œä½†è¿™é‡Œä¿æŒç®€å•
                cols[7] = new_status
                
                # é‡æ–°æ‹¼æ¥è¡Œ (æ³¨æ„ï¼šMarkdown è¡¨æ ¼çš„é¦–å°¾éœ€è¦ä¿ç•™ç©ºçš„ |)
                new_line = '|' + '|'.join(cols) + '|'
                
                # ä¸ºäº†ä¿æŒè¡¨æ ¼å¯¹é½ï¼Œéœ€è¦ç¡®ä¿æ¯åˆ—çš„å®½åº¦ä¸åŸ README åŒ¹é…ï¼Œ
                # ä½†æ‰‹åŠ¨ç»´æŠ¤å®½åº¦éå¸¸å¤æ‚ã€‚æˆ‘ä»¬è¿™é‡Œä½¿ç”¨ç®€å•çš„ '|' æ‹¼æ¥ï¼Œ
                # ä¾èµ– Markdown æ¸²æŸ“å™¨è‡ªåŠ¨è°ƒæ•´å¯¹é½ã€‚
                
                new_lines.append(new_line)
                continue # å·²å¤„ç†ï¼Œè·³è¿‡åç»­
        
        # éæ•°æ®è¡Œï¼ˆæ ‡é¢˜ã€åˆ†éš”çº¿ã€éè¡¨æ ¼å†…å®¹ç­‰ï¼‰ä¿æŒä¸å˜
        new_lines.append(line)

    return '\n'.join(new_lines)

def update_timestamp(content):
    """
    æ›´æ–° README é¡¶éƒ¨çš„â€œæ›´æ–°æ—¶é—´â€ã€‚
    æ³¨æ„ï¼šGitHub Actions é»˜è®¤è¿è¡Œåœ¨ UTC æ—¶é—´ï¼Œæ­¤æ—¶é—´ä¸º UTC æ—¶é—´ã€‚
    """
    
    # æ ¼å¼åŒ–å½“å‰æ—¶é—´ä¸º YYYY/MM/DD HH:MM
    # GitHub Actions è¿è¡Œåœ¨ UTC æ—¶é—´ï¼Œè¿™é‡Œè·å–çš„æ˜¯ UTC æ—¶é—´
    now = datetime.now() 
    current_time_str = now.strftime("%Y/%m/%d %H:%M")
    
    # æ­£åˆ™è¡¨è¾¾å¼ï¼š
    # ç›®æ ‡ï¼š(<p align="center">\s*<span>æ›´æ–°æ—¶é—´ï¼š) åé¢è·Ÿç€çš„å†…å®¹ (.*?) (</span>\s*</p>)
    # ä½¿ç”¨éè´ªå©ªåŒ¹é… (.*?) æ¥ç¡®ä¿åªæ›¿æ¢ span æ ‡ç­¾å†…çš„å†…å®¹
    pattern = r'(<p\s+align="center">\s*<span>æ›´æ–°æ—¶é—´ï¼š).*?(</span>\s*</p>)'
    
    # æ›¿æ¢å­—ç¬¦ä¸²ï¼š\1 æ˜¯åŒ¹é…åˆ°çš„ç¬¬ä¸€ä¸ªæ‹¬å·å†…å®¹ï¼Œç„¶åæ˜¯æ–°çš„æ—¶é—´ï¼Œ\2 æ˜¯ç¬¬äºŒä¸ªæ‹¬å·å†…å®¹
    replacement = r'\1' + current_time_str + r'\2'
    
    print(f"DEBUG: æ­£åœ¨æ›´æ–°æ—¶é—´æˆ³ä¸º: {current_time_str}")
    
    # æ‰§è¡Œæ›¿æ¢ (re.DOTALL ç¡®ä¿ . èƒ½åŒ¹é…æ¢è¡Œç¬¦ï¼Œä»¥é˜² span æ ‡ç­¾è·¨è¡Œ)
    updated_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
    
    return updated_content

def main():
    global update_results
    
    # 1. è·å–åŸå§‹ README å†…å®¹
    original_content = fetch_readme()
    if not original_content: return

    # 2. è§£æåšå®¢åˆ—è¡¨
    blogs = parse_blog_list(original_content)
    print(f"æ‰¾åˆ° {len(blogs)} ä¸ªåšå®¢ï¼Œå¼€å§‹æ£€æŸ¥æ›´æ–°...\n")
    
    # 3. æŠ“å–å¹¶å­˜å‚¨æ›´æ–°æ—¥æœŸ
    for blog in blogs:
        last_update = check_update(blog)
        # åªæœ‰æŠ“å–åˆ°åˆæ³•æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰æ‰æ›´æ–°å­—å…¸
        if last_update != "Unknown":
            update_results[blog['name']] = last_update
        
    print("\n" + "="*50)
    print(f"æ£€æŸ¥å®Œæˆï¼Œå…±æˆåŠŸè·å– {len(update_results)} ä¸ªåšå®¢çš„æ›´æ–°æ—¶é—´ã€‚")
    print("="*50)

    # 4. æ›´æ–° README å†…å®¹ (è¡¨æ ¼çŠ¶æ€)
    updated_content_table = original_content
    if update_results:
        print("å¼€å§‹æ›´æ–°è¡¨æ ¼çŠ¶æ€...")
        updated_content_table = update_readme_content(original_content, update_results)
    
    # 5. ã€æ–°å¢æ­¥éª¤ã€‘æ›´æ–°æ—¶é—´æˆ³
    print("å¼€å§‹æ›´æ–°é¡¶éƒ¨çš„è¿è¡Œæ—¶é—´æˆ³...")
    final_content = update_timestamp(updated_content_table)
    
    # 6. è¦†ç›–å†™å…¥åŸå§‹æ–‡ä»¶
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(final_content)
        
    print("\nâœ… README.md å·²æ›´æ–°ã€‚")

if __name__ == "__main__":
    main()